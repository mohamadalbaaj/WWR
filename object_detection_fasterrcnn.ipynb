{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_iou\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadeDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def load_annotations(self, annotation_file):\n",
    "        with open(annotation_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for shape in data[\"shapes\"]:\n",
    "            if shape[\"label\"] == \"window\":\n",
    "                points = shape[\"points\"]\n",
    "                \n",
    "                # Calculate bounding box from polygon points\n",
    "                x_coords = [p[0] for p in points]\n",
    "                y_coords = [p[1] for p in points]\n",
    "                x_min = min(x_coords)\n",
    "                y_min = min(y_coords)\n",
    "                x_max = max(x_coords)\n",
    "                y_max = max(y_coords)\n",
    "                \n",
    "                # Bounding box format: [x_min, y_min, width, height]\n",
    "                box = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "                boxes.append(box)\n",
    "                labels.append(1)  # Use 1 for \"window\" category label\n",
    "\n",
    "        return {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        annotation_path = os.path.join(self.annotations_dir, f\"{os.path.splitext(img_name)[0]}.json\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        annotations = self.load_annotations(annotation_path)\n",
    "\n",
    "        # Convert bounding boxes and labels to tensors\n",
    "        boxes = torch.tensor(annotations[\"boxes\"], dtype=torch.float32)\n",
    "        labels = torch.tensor(annotations[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "        if self.transform:  \n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Directories for training\n",
    "train_image_dir = os.path.join(\"ZJU_dataset\", \"images\")\n",
    "train_annotations_dir = os.path.join(\"ZJU_dataset\", \"annotation\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = FacadeDataset(train_image_dir, train_annotations_dir, transform=transform)\n",
    "\n",
    "# print(\"Dataset :\", dataset[0])\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proportions for train, validation, and test\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Load the full dataset (assuming train_annotations includes all data at this stage)\n",
    "full_dataset = FacadeDataset(train_image_dir, train_annotations_dir, transform=transform)\n",
    "\n",
    "# Calculate lengths of each split\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "val_size = int(val_ratio * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"Train dataset:\", len(train_dataset))\n",
    "print(\"Validation dataset:\", len(val_dataset))\n",
    "print(\"Test dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    # Retrieve image and target from dataset\n",
    "    image, target = dataset[idx]\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "    \n",
    "    # Convert the tensor image to a numpy array for visualization\n",
    "    image = image.permute(1, 2, 0).numpy()  # Change shape from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Add bounding boxes\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # Unpack the box coordinates\n",
    "        xmin, ymin, width, height = box\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        # Optional: Add label text if you have a mapping from `label` to class names\n",
    "        ax.text(xmin, ymin, f\"Label: {label.item()}\", color=\"white\", fontsize=8, backgroundcolor=\"red\")\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Test visualization with a sample from the train_dataset\n",
    "visualize_sample(train_dataset, idx=45)  # Change idx to visualize other samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "num_classes = 2  # Change this to the number of classes you have (including background)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a fucntion to convert the bounding boxes to x_min, y_min, x_max, y_max format instead of W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bounding_boxes(targets):\n",
    "    for target in targets:\n",
    "        boxes = target['boxes']\n",
    "        converted_boxes = []\n",
    "        for box in boxes:\n",
    "            x_min, y_min, width, height = box\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            converted_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        target['boxes'] = torch.tensor(converted_boxes, dtype=torch.float32)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if Cuda is avaliable and what GPU does it use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and print the device being used\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        targets = convert_bounding_boxes(targets)\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluation metrics on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_precisions, all_recalls, all_f1s, ious = [], [], [], []\n",
    "        \n",
    "        for images, targets in val_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            pred_boxes = [output['boxes'].cpu() for output in outputs]\n",
    "            targets = convert_bounding_boxes(targets)\n",
    "            target_boxes = [target['boxes'].cpu() for target in targets]\n",
    "\n",
    "            # print(\"Predicted Boxes:\", pred_boxes)\n",
    "            # print(\"targeted Boxes:\", target_boxes)\n",
    "            \n",
    "            # Calculate IoU for each image\n",
    "            batch_ious = []\n",
    "            for pb, tb in zip(pred_boxes, target_boxes):\n",
    "                if pb.size(0) > 0 and tb.size(0) > 0:\n",
    "                    iou = box_iou(pb, tb)\n",
    "                    batch_ious.append(iou.max(dim=1).values.mean().item())  # Mean IoU per prediction\n",
    "            ious.extend(batch_ious)\n",
    "            \n",
    "            # Calculate precision, recall, F1 at IoU 0.5 threshold\n",
    "            for pb, tb in zip(pred_boxes, target_boxes):\n",
    "                if pb.size(0) > 0 and tb.size(0) > 0:\n",
    "                    iou = box_iou(pb, tb)\n",
    "                    tp = (iou >= 0.5).sum().item()\n",
    "                    fp = pb.size(0) - tp\n",
    "                    fn = tb.size(0) - tp\n",
    "                    \n",
    "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    all_precisions.append(precision)\n",
    "                    all_recalls.append(recall)\n",
    "                    all_f1s.append(f1)\n",
    "\n",
    "        # Calculate mean metrics over the entire validation set\n",
    "        mean_iou = sum(ious) / len(ious) if ious else 0\n",
    "        mean_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0\n",
    "        mean_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0\n",
    "        mean_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0\n",
    "\n",
    "        print(f\"Validation Metrics - Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        print(f\"  Mean IoU: {mean_iou:.4f}\")\n",
    "        print(f\"  Precision at IoU 0.5: {mean_precision:.4f}\")\n",
    "        print(f\"  Recall at IoU 0.5: {mean_recall:.4f}\")\n",
    "        print(f\"  F1 Score at IoU 0.5: {mean_f1:.4f}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function for visualizing the results (predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(predictions, image_tensor, threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualizes predictions on an image.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): The predictions dictionary from Faster R-CNN, containing 'boxes', 'labels', and 'scores'.\n",
    "        image_tensor (Tensor): The image tensor.\n",
    "        threshold (float): Confidence threshold to filter boxes.\n",
    "        class_names (list): List of class names, where index corresponds to class label (optional).\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    img = transforms.ToPILImage()(image_tensor.cpu())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Draw each bounding box\n",
    "    boxes = predictions['boxes']\n",
    "    labels = predictions['labels']\n",
    "    scores = predictions['scores']\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:  # Filter boxes by score\n",
    "            x_min, y_min, x_max, y_max = box.detach().cpu().numpy()\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "\n",
    "            # Draw the rectangle\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get label name if provided, otherwise use label index\n",
    "            label_text = class_names[label] if class_names else f\"Class {label}\"\n",
    "            label_text = f\"{label_text}: {score:.2f}\"\n",
    "\n",
    "            # Add label and score\n",
    "            ax.text(x_min, y_min - 10, label_text, color='red', fontsize=12, backgroundcolor='white')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualzing the results (predictions) on the test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "class_names = [\"background\", \"window\"]  # Add your class names here, with \"background\" as the first item\n",
    "ind = 2  # Index of the image you want to visualize\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, _) in enumerate(test_loader):\n",
    "        images = [image.to(device) for image in images]\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Check if the desired index is within this batch\n",
    "        if ind >= batch_idx * len(images) and ind < (batch_idx + 1) * len(images):\n",
    "            relative_index = ind - batch_idx * len(images)  # Relative index within the current batch\n",
    "            print(f\"Visualizing predictions for image {ind}\")\n",
    "            visualize_predictions(predictions[relative_index], images[relative_index], threshold=0.5, class_names=class_names)\n",
    "            break  # Exit the loop after visualizing the desired image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the results (predictions) on the Evalution subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your image\n",
    "image_path = os.path.join(\"Evaluation_subset\", \"rectified_facade_DENW11AL0000h3LU.jpg\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Load the image\n",
    "input_image = load_image(image_path).unsqueeze(0)  # Add a batch dimension\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Run the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_image)\n",
    "\n",
    "# Visualize the predictions\n",
    "class_names = [\"background\", \"window\"]  # Adjust based on your class names\n",
    "visualize_predictions(predictions[0], input_image[0], threshold=0.5, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the model\n",
    "save_path = \"model_checkpoint.pth\"\n",
    "\n",
    "# Save the model and optimizer state dictionaries\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch + 1,  # Save the last completed epoch\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to load the model\n",
    "load_path = \"model_checkpoint.pth\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state dictionary\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Retrieve the last completed epoch\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "print(f\"Model loaded from {load_path}, starting from epoch {start_epoch}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
