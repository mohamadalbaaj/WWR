{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_iou\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import rich\n",
    "import pandas as pd\n",
    "from torchvision.ops import nms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model performance on the evalution dataset (test dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "num_classes = 2  # Change this to the number of classes you have (including background)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from fasterrcnn_checkpoint_batch_32.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path to load the model\n",
    "load_path = \"fasterrcnn_checkpoint_batch_32.pth\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "print(f\"Model loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "dataset_dir = \"Evaluation_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadeDataset(Dataset):\n",
    "    def __init__(self, dataset_dir, transform=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the merged annotation file\n",
    "        annotation_path = os.path.join(dataset_dir, \"merged.json\")\n",
    "        with open(annotation_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Map image_id to file_name\n",
    "        self.image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in data[\"images\"]}\n",
    "        \n",
    "        # Store annotations indexed by image_id\n",
    "        self.annotations = {}\n",
    "        for ann in data[\"annotations\"]:\n",
    "            image_id = ann[\"image_id\"]\n",
    "            if image_id not in self.annotations:\n",
    "                self.annotations[image_id] = []\n",
    "            self.annotations[image_id].append(ann)\n",
    "\n",
    "        # List of image filenames (ensuring they have annotations)\n",
    "        self.image_files = [file_name for img_id, file_name in self.image_id_to_filename.items() if img_id in self.annotations]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.dataset_dir, img_name)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Get the image ID\n",
    "        image_id = next(k for k, v in self.image_id_to_filename.items() if v == img_name)\n",
    "\n",
    "        # Retrieve annotation data for this image\n",
    "        annotations = self.annotations.get(image_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in annotations:\n",
    "            bbox = ann[\"bbox\"]  # COCO format: [x_min, y_min, width, height]\n",
    "            x_min, y_min, width, height = bbox\n",
    "            boxes.append([x_min, y_min, x_min + width, y_min + height])\n",
    "            labels.append(1)  # Assuming category_id 1 is for windows\n",
    "\n",
    "        # Convert bounding boxes and labels to tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        if self.transform:\n",
    "            for t in self.transform.transforms:\n",
    "                if isinstance(t, transforms.Resize):\n",
    "                    new_size = t.size\n",
    "                    if isinstance(new_size, int):  \n",
    "                        aspect_ratio = original_width / original_height\n",
    "                        new_width = new_size if original_width > original_height else int(new_size * aspect_ratio)\n",
    "                        new_height = new_size if original_width <= original_height else int(new_size / aspect_ratio)\n",
    "                    else:\n",
    "                        new_height, new_width = new_size\n",
    "\n",
    "                    scale_x, scale_y = new_width / original_width, new_height / original_height\n",
    "                    boxes[:, [0, 2]] *= scale_x  \n",
    "                    boxes[:, [1, 3]] *= scale_y  \n",
    "\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images with annotations: 477\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((image_new_dimension, image_new_dimension)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_dataset = FacadeDataset(dataset_dir, transform=transform)\n",
    "print(f\"Total images with annotations: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Mean IoU: 0.4091\n",
      "  Precision: 0.9169\n",
      "  Recall: 0.4209\n",
      "  F1 Score: 0.5164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Metric Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=\"Validating Metrics\", leave=False)\n",
    "    all_precisions, all_recalls, all_f1s, all_ious = [], [], [], []\n",
    "    for _, (images, targets) in enumerate(val_loader_tqdm):\n",
    "        images = [image.to(DEVICE) for image in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        pred_boxes = [output['boxes'].cpu() for output in outputs]\n",
    "        pred_confidences = [output['scores'].cpu() for output in outputs]\n",
    "        target_boxes = [target['boxes'].cpu() for target in targets]\n",
    "        \n",
    "        # Calculate precision, recall, F1 at IoU 0.5 threshold\n",
    "        for pb, pc, tb in zip(pred_boxes, pred_confidences, target_boxes):\n",
    "            # Step 1: Filter predictions by confidence\n",
    "            high_conf_mask = pc > threshold\n",
    "            pb = pb[high_conf_mask]\n",
    "            pc = pc[high_conf_mask]\n",
    "            \n",
    "            # Step 2: Apply Non-Maximum Suppression (NMS)\n",
    "            if pb.size(0) > 0:\n",
    "                indices = nms(pb, pc, iou_threshold=threshold)\n",
    "                pb = pb[indices]\n",
    "                pc = pc[indices]\n",
    "\n",
    "            # Step 3: Calculate IoU and True Positives\n",
    "            if pb.size(0) > 0 and tb.size(0) > 0:\n",
    "                init_iou = box_iou(pb, tb)\n",
    "                tp = (init_iou >= threshold).sum().item()  # True positives at IoU=0.5\n",
    "                fp = pb.size(0) - tp  # False positives\n",
    "                fn = tb.size(0) - tp  # False negatives\n",
    "                \n",
    "                iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                all_ious.append(iou)\n",
    "                all_precisions.append(precision)\n",
    "                all_recalls.append(recall)\n",
    "                all_f1s.append(f1)\n",
    "\n",
    "    # Calculate mean metrics over the entire validation set\n",
    "    mean_iou = sum(all_ious) / len(all_ious) if all_ious else 0\n",
    "    mean_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0\n",
    "    mean_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0\n",
    "    mean_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0\n",
    "\n",
    "    print(f\"  Mean IoU: {mean_iou:.4f}\")\n",
    "    print(f\"  Precision: {mean_precision:.4f}\")\n",
    "    print(f\"  Recall: {mean_recall:.4f}\")\n",
    "    print(f\"  F1 Score: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WWR calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your image\n",
    "file_name = \"rectified_facade_DENW11AL0000h3Gt.jpg\"\n",
    "\n",
    "image_path = os.path.join(\"Evaluation_subset\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "file_path = os.path.join(\"Evaluation_subset\", \"labels_facade_dataset_2024-06-09-08-43-50.json\")\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the correct device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Facade area in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GMLID: DENW11AL0000h3Gt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GMLID: DENW11AL0000h3Gt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Facade Height: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.081</span> meters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Facade Height: \u001b[1;36m9.081\u001b[0m meters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Facade Width: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.26536719358218</span> meters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Facade Width: \u001b[1;36m11.26536719358218\u001b[0m meters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Facade Area: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102.30079948491978</span> square meters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Facade Area: \u001b[1;36m102.30079948491978\u001b[0m square meters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = \"soest_duesseldorf_combined_valid_results_with_geometry_with_width.xlsx\"\n",
    "df = pd.read_excel(file_path, dtype=str)  # Read as strings to avoid type issues\n",
    "\n",
    "# Extract the gmlid from the filename\n",
    "gmlid = file_name.replace(\"rectified_facade_\", \"\").replace(\".jpg\", \"\")\n",
    "\n",
    "# Search for the corresponding row\n",
    "row = df[df[\"gmlid\"] == gmlid]\n",
    "\n",
    "if not row.empty:\n",
    "    # Retrieve relevant facade height and width\n",
    "    facade_height_meters = float(row[\"relevant_facade_height\"].values[0])\n",
    "    facade_width_meters = float(row[\"relevant_facade_width\"].values[0])\n",
    "\n",
    "    # Compute facade area\n",
    "    facade_area = facade_height_meters * facade_width_meters\n",
    "\n",
    "    rich.print(f\"GMLID: {gmlid}\")\n",
    "    rich.print(f\"Facade Height: {facade_height_meters} meters\")\n",
    "    rich.print(f\"Facade Width: {facade_width_meters} meters\")\n",
    "    rich.print(f\"Facade Area: {facade_area} square meters\")\n",
    "else:\n",
    "    rich.print(f\"No matching GMLID found for {gmlid}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### image area in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find the image details for the given file name\n",
    "image_details = next(item for item in data[\"images\"] if item[\"file_name\"] == file_name)\n",
    "\n",
    "# Extract width and height\n",
    "image_height_pixels = image_details[\"height\"]\n",
    "image_width_pixels = image_details[\"width\"]\n",
    "\n",
    "rich.print(f\"Image Height: {image_height_pixels} pixels\")\n",
    "rich.print(f\"Image Width: {image_width_pixels} pixels\")\n",
    "\n",
    "# Calculate the image area\n",
    "image_area_pixels = image_height_pixels * image_width_pixels\n",
    "\n",
    "# rich.print(f\"Image dimensions for '{file_name}': {image_width}x{image_height}\")\n",
    "rich.print(f\"Total image area for '{file_name}': {image_area_pixels} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pixel size in square meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pixel size in square meters\n",
    "pixel_size_meters = (facade_width_meters / image_width_pixels) * (\n",
    "        facade_height_meters / image_height_pixels)\n",
    "\n",
    "rich.print(\"pixel size in meters\", pixel_size_meters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ground truth area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = next(item[\"id\"] for item in data[\"images\"] if item[\"file_name\"] == file_name)\n",
    "\n",
    "# Step 2: Filter annotations for the image_id and category \"window\" (category_id = 1)\n",
    "window_annotations = [\n",
    "    annotation for annotation in data[\"annotations\"]\n",
    "    if annotation[\"image_id\"] == image_id and annotation[\"category_id\"] == 1\n",
    "]\n",
    "\n",
    "# Step 3: Calculate total area and count\n",
    "windows_area_gt = sum(ann[\"area\"] for ann in window_annotations)\n",
    "num_windows = len(window_annotations)\n",
    "\n",
    "# rich.print(f\"Number of windows: {num_windows}\")\n",
    "rich.print(f\"Total window area for '{file_name}': {windows_area_gt} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the predicted area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Load the image\n",
    "input_image = load_image(image_path).unsqueeze(0)  # Add a batch dimension\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Run the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_image)\n",
    "    # rich.print(\"predictions\", predictions)\n",
    "\n",
    "# Extract predictions from the output\n",
    "boxes = predictions[0]['boxes']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "# Filter boxes by scores > 0.5\n",
    "threshold = 0.1\n",
    "valid_boxes = boxes[scores > threshold]\n",
    "\n",
    "# Get the count of valid boxes\n",
    "num_valid_boxes = valid_boxes.shape[0]\n",
    "\n",
    "# Calculate the area of each valid bounding box\n",
    "areas = (valid_boxes[:, 2] - valid_boxes[:, 0]) * (valid_boxes[:, 3] - valid_boxes[:, 1])\n",
    "\n",
    "# Sum the areas to get windows_area\n",
    "windows_area_p = areas.sum().item()\n",
    "\n",
    "# Print the total windows area\n",
    "# rich.print(f\"Number of valid boxes: {num_valid_boxes}\")\n",
    "rich.print(f\"Total predicted windows area for '{file_name}': {windows_area_p} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR actual and predicted in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ground truth WWR\n",
    "wwr_gt = windows_area_gt / image_area_pixels if image_area_pixels > 0 else 0\n",
    "\n",
    "rich.print(f\"Window-to-Wall Ratio (ground truth) (WWR) for '{file_name}': {wwr_gt:.4f}\")\n",
    "\n",
    "# Calculate predicted WWR\n",
    "wwr_P = windows_area_p / image_area_pixels if image_area_pixels > 0 else 0\n",
    "\n",
    "rich.print(f\"Window-to-Wall Ratio (predcited) (WWR) for '{file_name}': {wwr_P:.4f}\")\n",
    "\n",
    "# Calculate the absolute difference\n",
    "wwr_difference = abs(wwr_gt - wwr_P)\n",
    "\n",
    "# Optionally, calculate percentage difference\n",
    "wwr_percentage_diff = (wwr_difference / wwr_gt * 100) if wwr_gt > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "# rich.print(f\"[bold]Difference between ground truth and predicted WWR:[/bold] {wwr_difference:.4f}\")\n",
    "# rich.print(f\"[bold]Percentage difference:[/bold] {wwr_percentage_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR actual in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade_meters = image_area_pixels * pixel_size_meters\n",
    "\n",
    "rich.print(f\"facade in meters {facade_meters} meters²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_meters_GT = windows_area_gt * pixel_size_meters\n",
    "\n",
    "rich.print(f\"windows in meters ground trught {window_meters_GT} meters²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WWR_GT_actual = window_meters_GT / facade_meters\n",
    "\n",
    "rich.print(f\"WWR Ground trugth {WWR_GT_actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR predicted in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_meters_P = windows_area_p * pixel_size_meters\n",
    "\n",
    "rich.print(f\"windows in meters predcited {window_meters_P} meters²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WWR_p_actual = window_meters_P / facade_meters\n",
    "\n",
    "rich.print(f\"WWR predicted {WWR_p_actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the results (Faster RCNN) on the Evalution subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(predictions, image_tensor, threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualizes predictions on an image.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): The predictions dictionary from Faster R-CNN, containing 'boxes', 'labels', and 'scores'.\n",
    "        image_tensor (Tensor): The image tensor.\n",
    "        threshold (float): Confidence threshold to filter boxes.\n",
    "        class_names (list): List of class names, where index corresponds to class label (optional).\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    img = transforms.ToPILImage()(image_tensor.cpu())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Draw each bounding box\n",
    "    boxes = predictions['boxes']\n",
    "    labels = predictions['labels']\n",
    "    scores = predictions['scores']\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:  # Filter boxes by score\n",
    "            x_min, y_min, x_max, y_max = box.detach().cpu().numpy()\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "\n",
    "            # Draw the rectangle\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get label name if provided, otherwise use label index\n",
    "            label_text = class_names[label] if class_names else f\"Class {label}\"\n",
    "            label_text = f\"{label_text}: {score:.2f}\"\n",
    "\n",
    "            # Add label and score\n",
    "            ax.text(x_min, y_min - 10, label_text, color='red', fontsize=12, backgroundcolor='white')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the correct device\n",
    "model = model.to(device)\n",
    "# Define the path to your image\n",
    "image_path = os.path.join(\"Evaluation_subset\", \"rectified_facade_DENW11AL0000h3Gt.jpg\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Load the image\n",
    "input_image = load_image(image_path).unsqueeze(0)  # Add a batch dimension\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Run the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_image)\n",
    "    # rich.print(\"predictions\", predictions)\n",
    "\n",
    "# Visualize the predictions\n",
    "class_names = [\"background\", \"window\"]  # Adjust based on your class names\n",
    "visualize_predictions(predictions[0], input_image[0], threshold=0.1, class_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
