{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.ops import nms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import rich\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacadeDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def load_annotations(self, annotation_file):\n",
    "        with open(annotation_file) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for shape in data[\"shapes\"]:\n",
    "            if shape[\"label\"] == \"window\":\n",
    "                points = shape[\"points\"]\n",
    "                \n",
    "                # Calculate bounding box from polygon points\n",
    "                x_coords = [p[0] for p in points]\n",
    "                y_coords = [p[1] for p in points]\n",
    "                x_min = min(x_coords)\n",
    "                y_min = min(y_coords)\n",
    "                x_max = max(x_coords)\n",
    "                y_max = max(y_coords)\n",
    "                \n",
    "                # Bounding box format: [x_min, y_min, width, height]\n",
    "                box = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "                boxes.append(box)\n",
    "                labels.append(1)  # Use 1 for \"window\" category label\n",
    "\n",
    "        return {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        annotation_path = os.path.join(self.annotations_dir, f\"{os.path.splitext(img_name)[0]}.json\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        annotations = self.load_annotations(annotation_path)\n",
    "\n",
    "        # Convert bounding boxes and labels to tensors\n",
    "        boxes = torch.tensor(annotations[\"boxes\"], dtype=torch.float32)\n",
    "        labels = torch.tensor(annotations[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "        if self.transform:  \n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load original dataset\n",
    "train_image_dir = os.path.join(\"ZJU_dataset_2\", \"images\")\n",
    "train_annotations_dir = os.path.join(\"ZJU_dataset_2\", \"annotation\")\n",
    "original_dataset = FacadeDataset(train_image_dir, train_annotations_dir, transform=transform)\n",
    "\n",
    "# Load augmented dataset\n",
    "augmented_image_dir = os.path.join(\"ZJU_dataset_augmented\", \"images\")\n",
    "augmented_annotations_dir = os.path.join(\"ZJU_dataset_augmented\", \"annotation\")\n",
    "augmented_dataset = FacadeDataset(augmented_image_dir, augmented_annotations_dir, transform=transform)\n",
    "\n",
    "# Combine both datasets\n",
    "dataset = torch.utils.data.ConcatDataset([original_dataset, augmented_dataset])\n",
    "\n",
    "print(\"Total dataset length after augmentation:\", len(dataset))\n",
    "print(\"type\", type(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    # Retrieve image and target from dataset\n",
    "    image, target = dataset[idx]\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "    \n",
    "    # Convert the tensor image to a numpy array for visualization\n",
    "    image = image.permute(1, 2, 0).numpy()  # Change shape from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Add bounding boxes\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # Unpack the box coordinates\n",
    "        xmin, ymin, width, height = box\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        # Optional: Add label text if you have a mapping from `label` to class names\n",
    "        ax.text(xmin, ymin, f\"Label: {label.item()}\", color=\"white\", fontsize=8, backgroundcolor=\"red\")\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Test visualization with a sample from the train_dataset\n",
    "# visualize_sample(train_dataset, idx=45)  # Change idx to visualize other samples\n",
    "visualize_sample(dataset, idx=250)  # Change idx to visualize other samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proportions for train, validation, and test\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Load the full dataset (assuming train_annotations includes all data at this stage)\n",
    "# full_dataset = FacadeDataset(train_image_dir, train_annotations_dir, transform=transform)\n",
    "full_dataset = dataset\n",
    "\n",
    "# Calculate lengths of each split\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "val_size = int(val_ratio * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(\"Train dataset:\", len(train_dataset))\n",
    "print(\"Validation dataset:\", len(val_dataset))\n",
    "print(\"Test dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch from the dataloader\n",
    "for batch in train_loader:\n",
    "    images, labels = batch  # Assuming dataset returns (image, label) pairs\n",
    "    \n",
    "    print(f\"Number of images in batch: {len(images)}\")\n",
    "    \n",
    "    # Print the size of each image in the batch\n",
    "    for i, img in enumerate(images):\n",
    "        print(f\"Image {i+1} size: {img.shape}\")  # (Channels, Height, Width)\n",
    "    \n",
    "    break  # Exit after first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx):\n",
    "    # Retrieve image and target from dataset\n",
    "    image, target = dataset[idx]\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "    \n",
    "    # Convert the tensor image to a numpy array for visualization\n",
    "    image = image.permute(1, 2, 0).numpy()  # Change shape from (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot the image\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Add bounding boxes\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # Unpack the box coordinates\n",
    "        xmin, ymin, width, height = box\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        # Optional: Add label text if you have a mapping from `label` to class names\n",
    "        ax.text(xmin, ymin, f\"Label: {label.item()}\", color=\"white\", fontsize=8, backgroundcolor=\"red\")\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Test visualization with a sample from the train_dataset\n",
    "# visualize_sample(train_dataset, idx=45)  # Change idx to visualize other samples\n",
    "visualize_sample(train_dataset, idx=45)  # Change idx to visualize other samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "num_classes = 2  # Change this to the number of classes you have (including background)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a fucntion to convert the bounding boxes to x_min, y_min, x_max, y_max format instead of W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bounding_boxes(targets):\n",
    "    for target in targets:\n",
    "        boxes = target['boxes']\n",
    "        converted_boxes = []\n",
    "        for box in boxes:\n",
    "            x_min, y_min, width, height = box\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            converted_boxes.append([x_min, y_min, x_max, y_max])\n",
    "        target['boxes'] = torch.tensor(converted_boxes, dtype=torch.float32)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if Cuda is avaliable and what GPU does it use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and print the device being used\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    param_grid, \n",
    "    device='cuda', \n",
    "    num_epochs=1, \n",
    "    confidence_threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Find the best hyperparameters for the model using grid search.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The model to be trained and evaluated.\n",
    "        train_loader: DataLoader for the training set.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        param_grid: Dictionary of hyperparameters to search. Example:\n",
    "                    {\n",
    "                        'lr': [0.001, 0.005],\n",
    "                        'momentum': [0.9, 0.95],\n",
    "                        'weight_decay': [0.0001, 0.0005]\n",
    "                    }\n",
    "        device: Device to use ('cuda' or 'cpu').\n",
    "        num_epochs: Number of epochs for each hyperparameter combination.\n",
    "        confidence_threshold: Confidence threshold for predictions.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with the best hyperparameters and their corresponding metrics.\n",
    "    \"\"\"\n",
    "    from itertools import product\n",
    "    import torch.optim as optim\n",
    "\n",
    "    best_params = None\n",
    "    best_metrics = None\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    for params in param_combinations:\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "        \n",
    "        # Update model and optimizer\n",
    "        model_copy = model.to(device)  # Ensure model is on the correct device\n",
    "        optimizer = optim.SGD(\n",
    "            model_copy.parameters(),\n",
    "            lr=params['lr'],\n",
    "            momentum=params['momentum'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Training loop (1 epoch per combination)\n",
    "        model_copy.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for images, targets in train_loader:\n",
    "                targets = convert_bounding_boxes(targets)\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss_dict = model_copy(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model_copy.eval()\n",
    "        with torch.no_grad():\n",
    "            all_precisions, all_recalls, all_f1s, ious = [], [], [], []\n",
    "\n",
    "            for images, targets in val_loader:\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model_copy(images)\n",
    "                pred_boxes = [output['boxes'].cpu() for output in outputs]\n",
    "                pred_confidences = [output['scores'].cpu() for output in outputs]\n",
    "                target_boxes = [target['boxes'].cpu() for target in targets]\n",
    "\n",
    "                # Calculate metrics\n",
    "                for pb, pc, tb in zip(pred_boxes, pred_confidences, target_boxes):\n",
    "                    high_conf_mask = pc > confidence_threshold\n",
    "                    pb = pb[high_conf_mask]\n",
    "                    pc = pc[high_conf_mask]\n",
    "\n",
    "                    if pb.size(0) > 0:\n",
    "                        indices = nms(pb, pc, iou_threshold=0.5)\n",
    "                        pb = pb[indices]\n",
    "\n",
    "                    if pb.size(0) > 0 and tb.size(0) > 0:\n",
    "                        iou = box_iou(pb, tb)\n",
    "                        tp = (iou >= 0.5).sum().item()\n",
    "                        fp = pb.size(0) - tp\n",
    "                        fn = tb.size(0) - tp\n",
    "\n",
    "                        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "                        all_precisions.append(precision)\n",
    "                        all_recalls.append(recall)\n",
    "                        all_f1s.append(f1)\n",
    "\n",
    "            mean_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0\n",
    "            mean_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0\n",
    "            mean_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0\n",
    "\n",
    "            metrics = {\n",
    "                'precision': mean_precision,\n",
    "                'recall': mean_recall,\n",
    "                'f1': mean_f1\n",
    "            }\n",
    "        \n",
    "        print(f\"Metrics for {params}: {metrics}\")\n",
    "\n",
    "        if best_metrics is None or metrics['f1'] > best_metrics['f1']:\n",
    "            best_params = params\n",
    "            best_metrics = metrics\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    print(f\"Best metrics: {best_metrics}\")\n",
    "    return {'best_params': best_params, 'best_metrics': best_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.005],\n",
    "    'momentum': [0.9, 0.95],\n",
    "    'weight_decay': [0.0001, 0.0005]\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_hyperparams = find_best_hyperparameters(model, train_loader, val_loader, param_grid, device=device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### early stopping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/Bjarten/early-stopping-pytorch\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.epoch = 0\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, epoch, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = Averager()\n",
    "totalTrainLosses = []\n",
    "val_losses = Averager()\n",
    "totalValLosses = []\n",
    "lowest_val_loss = float('inf')\n",
    "early_stopping = EarlyStopping(patience = 3, path = \"checkpoint.pth\", verbose=True)\n",
    "# has to be in train mode for both train and valid coz the outputs are different in two cases\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_losses.reset()\n",
    "    val_losses.reset()\n",
    "    model.train()\n",
    "    # Progress bar for the training loop\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_index, (images, targets) in enumerate(train_loader_tqdm):\n",
    "        targets = convert_bounding_boxes(targets)\n",
    "        # move the images and targets to device\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        # track the loss\n",
    "        train_losses.send(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {batch_index} Loss: {loss.item()}\")\n",
    "\n",
    "    # evaluate\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        # Progress bar for the validation loop\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=\"Validating loss\", leave=False)\n",
    "        for _, (images, targets) in enumerate(val_loader_tqdm):\n",
    "            targets = convert_bounding_boxes(targets)\n",
    "            # move the images and targets to device\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_loss = sum(loss for loss in val_loss_dict.values())\n",
    "\n",
    "            # track the loss\n",
    "            val_losses.send(val_loss.item())\n",
    "\n",
    "    # Metric Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=\"Validating Metrics\", leave=False)\n",
    "        all_precisions, all_recalls, all_f1s, ious = [], [], [], []\n",
    "        for _, (images, targets) in enumerate(val_loader_tqdm):\n",
    "            images = [image.to(DEVICE) for image in images]\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            pred_boxes = [output['boxes'].cpu() for output in outputs]\n",
    "            pred_confidences = [output['scores'].cpu() for output in outputs]\n",
    "            targets = convert_bounding_boxes(targets)\n",
    "            target_boxes = [target['boxes'].cpu() for target in targets]\n",
    "\n",
    "            # print(\"Predicted Boxes:\", pred_boxes)\n",
    "            # print(\"targeted Boxes:\", target_boxes)\n",
    "            \n",
    "            # Calculate IoU for each image\n",
    "            batch_ious = []\n",
    "            for pb, tb in zip(pred_boxes, target_boxes):\n",
    "                if pb.size(0) > 0 and tb.size(0) > 0:\n",
    "                    iou = box_iou(pb, tb)\n",
    "                    batch_ious.append(iou.max(dim=1).values.mean().item())  # Mean IoU per prediction\n",
    "            ious.extend(batch_ious)\n",
    "            \n",
    "            # Calculate precision, recall, F1 at IoU 0.5 threshold\n",
    "            for pb, pc, tb in zip(pred_boxes, pred_confidences, target_boxes):\n",
    "                # Step 1: Filter predictions by confidence\n",
    "                high_conf_mask = pc > confidence_threshold\n",
    "                pb = pb[high_conf_mask]\n",
    "                pc = pc[high_conf_mask]\n",
    "                \n",
    "                # Step 2: Apply Non-Maximum Suppression (NMS)\n",
    "                if pb.size(0) > 0:\n",
    "                    indices = nms(pb, pc, iou_threshold=0.5)\n",
    "                    pb = pb[indices]\n",
    "                    pc = pc[indices]\n",
    "\n",
    "                # Step 3: Calculate IoU and True Positives\n",
    "                if pb.size(0) > 0 and tb.size(0) > 0:\n",
    "                    iou = box_iou(pb, tb)\n",
    "                    tp = (iou >= 0.5).sum().item()  # True positives at IoU=0.5\n",
    "                    fp = pb.size(0) - tp  # False positives\n",
    "                    fn = tb.size(0) - tp  # False negatives\n",
    "                    \n",
    "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    all_precisions.append(precision)\n",
    "                    all_recalls.append(recall)\n",
    "                    all_f1s.append(f1)\n",
    "\n",
    "        # Calculate mean metrics over the entire validation set\n",
    "        mean_iou = sum(ious) / len(ious) if ious else 0\n",
    "        mean_precision = sum(all_precisions) / len(all_precisions) if all_precisions else 0\n",
    "        mean_recall = sum(all_recalls) / len(all_recalls) if all_recalls else 0\n",
    "        mean_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0\n",
    "\n",
    "        print(f\"Validation Metrics - Epoch [{epoch + 1}/{NUM_EPOCHS}]\")\n",
    "        print(f\"  Mean IoU: {mean_iou:.4f}\")\n",
    "        print(f\"  Precision: {mean_precision:.4f}\")\n",
    "        print(f\"  Recall: {mean_recall:.4f}\")\n",
    "        print(f\"  F1 Score: {mean_f1:.4f}\")\n",
    "\n",
    "    if val_losses.value >= lowest_val_loss and lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # print stats\n",
    "    print(f\"Epoch #{epoch} TRAIN LOSS: {train_losses.value} VALIDATION LOSS: {val_losses.value}\\n\")\n",
    "    totalTrainLosses.append(train_losses.value)\n",
    "    totalValLosses.append(val_losses.value)\n",
    "    early_stopping(val_losses.value, epoch, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "plt.plot(totalTrainLosses, label = \"Training Loss\", color = \"deepskyblue\")\n",
    "plt.plot(totalValLosses, label = \"Validation Loss\", color = \"darkorange\")\n",
    "plt.axvline(early_stopping.epoch, label = \"Early Stop\", color = \"red\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.show()\n",
    "os.rename(\"checkpoint.pth\", \"fasterrcnn_checkpoint_2.pth\")\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function for visualizing the results (predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(predictions, image_tensor, threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualizes predictions on an image.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): The predictions dictionary from Faster R-CNN, containing 'boxes', 'labels', and 'scores'.\n",
    "        image_tensor (Tensor): The image tensor.\n",
    "        threshold (float): Confidence threshold to filter boxes.\n",
    "        class_names (list): List of class names, where index corresponds to class label (optional).\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    img = transforms.ToPILImage()(image_tensor.cpu())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Draw each bounding box\n",
    "    boxes = predictions['boxes']\n",
    "    labels = predictions['labels']\n",
    "    scores = predictions['scores']\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:  # Filter boxes by score\n",
    "            x_min, y_min, x_max, y_max = box.detach().cpu().numpy()\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "\n",
    "            # Draw the rectangle\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get label name if provided, otherwise use label index\n",
    "            label_text = class_names[label] if class_names else f\"Class {label}\"\n",
    "            label_text = f\"{label_text}: {score:.2f}\"\n",
    "\n",
    "            # Add label and score\n",
    "            ax.text(x_min, y_min - 10, label_text, color='red', fontsize=12, backgroundcolor='white')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "class_names = [\"background\", \"window\"]  # Add your class names here, with \"background\" as the first item\n",
    "ind = 6  # Index of the image you want to visualize\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, _) in enumerate(test_loader):\n",
    "        images = [image.to(DEVICE) for image in images]\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Check if the desired index is within this batch\n",
    "        if ind >= batch_idx * len(images) and ind < (batch_idx + 1) * len(images):\n",
    "            relative_index = ind - batch_idx * len(images)  # Relative index within the current batch\n",
    "            print(f\"Visualizing predictions for image {ind}\")\n",
    "            visualize_predictions(predictions[relative_index], images[relative_index], threshold=0.5, class_names=class_names)\n",
    "            break  # Exit the loop after visualizing the desired image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define the path to save the model\n",
    "save_path = \"fasterrcnn_checkpoint.pth\"\n",
    "\n",
    "# Save the model and optimizer state dictionaries\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch + 1,  # Save the last completed epoch\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
