{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_iou\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "num_classes = 2  # Change this to the number of classes you have (including background)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from fasterrcnn_checkpoint.pth, starting from epoch 1\n"
     ]
    }
   ],
   "source": [
    "# Define the path to load the model\n",
    "load_path = \"fasterrcnn_checkpoint.pth\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(load_path)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Load the optimizer state dictionary\n",
    "#  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Retrieve the last completed epoch\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "print(f\"Model loaded from {load_path}, starting from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculate the predicted area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total predicted windows area for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11250.85546875</span> pixels²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total predicted windows area for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m11250.85546875\u001b[0m pixels²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the correct device\n",
    "model = model.to(device)\n",
    "# Define the path to your image\n",
    "file_name = \"rectified_facade_DENW11AL0000h3Gt.jpg\"\n",
    "\n",
    "image_path = os.path.join(\"Evaluation_subset\", file_name)\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Load the image\n",
    "input_image = load_image(image_path).unsqueeze(0)  # Add a batch dimension\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Run the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_image)\n",
    "    # rich.print(\"predictions\", predictions)\n",
    "\n",
    "# Extract predictions from the output\n",
    "boxes = predictions[0]['boxes']\n",
    "scores = predictions[0]['scores']\n",
    "\n",
    "# Filter boxes by scores > 0.5\n",
    "threshold = 0.5\n",
    "valid_boxes = boxes[scores > threshold]\n",
    "\n",
    "# Get the count of valid boxes\n",
    "num_valid_boxes = valid_boxes.shape[0]\n",
    "\n",
    "# Calculate the area of each valid bounding box\n",
    "areas = (valid_boxes[:, 2] - valid_boxes[:, 0]) * (valid_boxes[:, 3] - valid_boxes[:, 1])\n",
    "\n",
    "# Sum the areas to get windows_area\n",
    "windows_area_p = areas.sum().item()\n",
    "\n",
    "# Print the total windows area\n",
    "# rich.print(f\"Number of valid boxes: {num_valid_boxes}\")\n",
    "rich.print(f\"Total predicted windows area for '{file_name}': {windows_area_p} pixels²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ground truth area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total window area for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21784.693692249522</span> pixels²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total window area for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m21784.693692249522\u001b[0m pixels²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "file_path = os.path.join(\"Evaluation_subset\", \"labels_facade_dataset_2024-06-09-08-43-50.json\")\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "image_id = next(item[\"id\"] for item in data[\"images\"] if item[\"file_name\"] == file_name)\n",
    "\n",
    "# Step 2: Filter annotations for the image_id and category \"window\" (category_id = 1)\n",
    "window_annotations = [\n",
    "    annotation for annotation in data[\"annotations\"]\n",
    "    if annotation[\"image_id\"] == image_id and annotation[\"category_id\"] == 1\n",
    "]\n",
    "\n",
    "# Step 3: Calculate total area and count\n",
    "windows_area_gt = sum(ann[\"area\"] for ann in window_annotations)\n",
    "num_windows = len(window_annotations)\n",
    "\n",
    "# rich.print(f\"Number of windows: {num_windows}\")\n",
    "rich.print(f\"Total window area for '{file_name}': {windows_area_gt} pixels²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### image area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total image area for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91664</span> pixels²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total image area for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m91664\u001b[0m pixels²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Find the image details for the given file name\n",
    "image_details = next(item for item in data[\"images\"] if item[\"file_name\"] == file_name)\n",
    "\n",
    "# Extract width and height\n",
    "image_width = image_details[\"width\"]\n",
    "image_height = image_details[\"height\"]\n",
    "\n",
    "# Calculate the image area\n",
    "image_area = image_width * image_height\n",
    "\n",
    "# rich.print(f\"Image dimensions for '{file_name}': {image_width}x{image_height}\")\n",
    "rich.print(f\"Total image area for '{file_name}': {image_area} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Window-to-Wall Ratio <span style=\"font-weight: bold\">(</span>ground truth<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>WWR<span style=\"font-weight: bold\">)</span> for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2377</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Window-to-Wall Ratio \u001b[1m(\u001b[0mground truth\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mWWR\u001b[1m)\u001b[0m for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m0.2377\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Window-to-Wall Ratio <span style=\"font-weight: bold\">(</span>predcited<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>WWR<span style=\"font-weight: bold\">)</span> for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1227</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Window-to-Wall Ratio \u001b[1m(\u001b[0mpredcited\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mWWR\u001b[1m)\u001b[0m for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m0.1227\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Percentage difference:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.35</span>%\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPercentage difference:\u001b[0m \u001b[1;36m48.35\u001b[0m%\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate ground truth WWR\n",
    "wwr_gt = windows_area_gt / image_area if image_area > 0 else 0\n",
    "\n",
    "rich.print(f\"Window-to-Wall Ratio (ground truth) (WWR) for '{file_name}': {wwr_gt:.4f}\")\n",
    "\n",
    "# Calculate predicted WWR\n",
    "wwr_P = windows_area_p / image_area if image_area > 0 else 0\n",
    "\n",
    "rich.print(f\"Window-to-Wall Ratio (predcited) (WWR) for '{file_name}': {wwr_P:.4f}\")\n",
    "\n",
    "# Calculate the absolute difference\n",
    "wwr_difference = abs(wwr_gt - wwr_P)\n",
    "\n",
    "# Optionally, calculate percentage difference\n",
    "wwr_percentage_diff = (wwr_difference / wwr_gt * 100) if wwr_gt > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "# rich.print(f\"[bold]Difference between ground truth and predicted WWR:[/bold] {wwr_difference:.4f}\")\n",
    "rich.print(f\"[bold]Percentage difference:[/bold] {wwr_percentage_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the results (Faster RCNN) on the Evalution subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(predictions, image_tensor, threshold=0.5, class_names=None):\n",
    "    \"\"\"\n",
    "    Visualizes predictions on an image.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): The predictions dictionary from Faster R-CNN, containing 'boxes', 'labels', and 'scores'.\n",
    "        image_tensor (Tensor): The image tensor.\n",
    "        threshold (float): Confidence threshold to filter boxes.\n",
    "        class_names (list): List of class names, where index corresponds to class label (optional).\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    img = transforms.ToPILImage()(image_tensor.cpu())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Draw each bounding box\n",
    "    boxes = predictions['boxes']\n",
    "    labels = predictions['labels']\n",
    "    scores = predictions['scores']\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:  # Filter boxes by score\n",
    "            x_min, y_min, x_max, y_max = box.detach().cpu().numpy()\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "\n",
    "            # Draw the rectangle\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get label name if provided, otherwise use label index\n",
    "            label_text = class_names[label] if class_names else f\"Class {label}\"\n",
    "            label_text = f\"{label_text}: {score:.2f}\"\n",
    "\n",
    "            # Add label and score\n",
    "            ax.text(x_min, y_min - 10, label_text, color='red', fontsize=12, backgroundcolor='white')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the correct device\n",
    "model = model.to(device)\n",
    "# Define the path to your image\n",
    "image_path = os.path.join(\"Evaluation_subset\", \"rectified_facade_DENW11AL0000h3Gt.jpg\")\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "# Load the image\n",
    "input_image = load_image(image_path).unsqueeze(0)  # Add a batch dimension\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "# Run the model on the image\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_image)\n",
    "    # rich.print(\"predictions\", predictions)\n",
    "\n",
    "# Visualize the predictions\n",
    "class_names = [\"background\", \"window\"]  # Adjust based on your class names\n",
    "visualize_predictions(predictions[0], input_image[0], threshold=0.5, class_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
