{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_iou\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import rich\n",
    "import pandas as pd\n",
    "from torchvision.ops import nms\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create dataset to fit the yolo requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths for input data\n",
    "image_dir = \"Evaluation_dataset\"\n",
    "annotation_path = os.path.join(\"Evaluation_dataset\", \"merged.json\")  # Single JSON file\n",
    "\n",
    "# Paths for YOLO-formatted dataset (only validation)\n",
    "output_dir = \"datasets\"\n",
    "val_images_dir = os.path.join(output_dir, \"val\", \"images\")\n",
    "val_labels_dir = os.path.join(output_dir, \"val\", \"labels\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(val_images_dir, exist_ok=True)\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "\n",
    "# Load COCO-style annotation file\n",
    "with open(annotation_path) as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Create a mapping from image ID to file name and dimensions\n",
    "image_info = {img[\"id\"]: {\"file_name\": img[\"file_name\"], \"width\": img[\"width\"], \"height\": img[\"height\"]}\n",
    "              for img in coco_data[\"images\"]}\n",
    "\n",
    "# Organize annotations by image ID\n",
    "annotations_by_image = {}\n",
    "for ann in coco_data[\"annotations\"]:\n",
    "    image_id = ann[\"image_id\"]\n",
    "    if image_id not in annotations_by_image:\n",
    "        annotations_by_image[image_id] = []\n",
    "    \n",
    "    # Convert COCO bbox to YOLO format\n",
    "    x_min, y_min, bbox_width, bbox_height = ann[\"bbox\"]\n",
    "    img_width = image_info[image_id][\"width\"]\n",
    "    img_height = image_info[image_id][\"height\"]\n",
    "\n",
    "    center_x = (x_min + bbox_width / 2) / img_width\n",
    "    center_y = (y_min + bbox_height / 2) / img_height\n",
    "    norm_width = bbox_width / img_width\n",
    "    norm_height = bbox_height / img_height\n",
    "\n",
    "    # YOLO format: [class_id, center_x, center_y, width, height]\n",
    "    yolo_annotation = f\"0 {center_x} {center_y} {norm_width} {norm_height}\"\n",
    "    annotations_by_image[image_id].append(yolo_annotation)\n",
    "\n",
    "# Process and save all images to the val dataset\n",
    "for image_id, img_data in image_info.items():\n",
    "    img_file = img_data[\"file_name\"]\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "    label_path = os.path.join(val_labels_dir, f\"{os.path.splitext(img_file)[0]}.txt\")\n",
    "\n",
    "    # Copy image to val directory\n",
    "    shutil.copy(img_path, val_images_dir)\n",
    "\n",
    "    # Save YOLO annotations\n",
    "    yolo_annotations = annotations_by_image.get(image_id, [])\n",
    "    with open(label_path, \"w\") as label_file:\n",
    "        label_file.write(\"\\n\".join(yolo_annotations))\n",
    "\n",
    "print(f\"Dataset organized successfully! All {len(image_info)} images are in the 'val' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model performance on the evalution dataset (test dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the custom-trained weights\n",
    "model = YOLO('trained_models/YOLOv8_training/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.58  Python-3.10.0 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\mohamad\\WWR\\datasets\\val\\labels.cache... 477 images, 0 backgrounds, 0 corrupt: 100%|██████████| 477/477 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 30/30 [00:04<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        477       5233      0.641      0.606      0.626       0.32\n",
      "Speed: 0.1ms preprocess, 2.9ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val25\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "full_results = model.val(\n",
    "    data=\"yolo_data.yaml\",         # Path to data.yaml or dict specifying train/val paths\n",
    "    iou = 0.5,     # Sets the Intersection Over Union (IoU) threshold for Non-Maximum Suppression (NMS).\n",
    "    device = \"cuda:0\"\n",
    ")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision: 0.3203403753663233\n",
      "Mean average precision at IoU=0.50: 0.6257152877070741\n",
      "Precision: [    0.64146]\n",
      "Recall: [    0.60577]\n",
      "F1 score: [    0.62311]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean average precision:\", full_results.box.map)\n",
    "print(\"Mean average precision at IoU=0.50:\", full_results.box.map50)\n",
    "print(\"Precision:\", full_results.box.p)\n",
    "print(\"Recall:\", full_results.box.r)\n",
    "print(\"F1 score:\", full_results.box.f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WWR calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your image\n",
    "file_name = \"rectified_facade_DENW11AL0000h3Gt.jpg\"\n",
    "\n",
    "image_path = os.path.join(\"Evaluation_subset\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "file_path = os.path.join(\"Evaluation_subset\", \"labels_facade_dataset_2024-06-09-08-43-50.json\")\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the correct device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Facade area in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GMLID: DENW11AL0000h3Gt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GMLID: DENW11AL0000h3Gt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Facade Height: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.081</span> meters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Facade Height: \u001b[1;36m9.081\u001b[0m meters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Facade Width: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.26536719358218</span> meters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Facade Width: \u001b[1;36m11.26536719358218\u001b[0m meters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Facade Area: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102.30079948491978</span> square meters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Facade Area: \u001b[1;36m102.30079948491978\u001b[0m square meters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = \"soest_duesseldorf_combined_valid_results_with_geometry_with_width.xlsx\"\n",
    "df = pd.read_excel(file_path, dtype=str)  # Read as strings to avoid type issues\n",
    "\n",
    "# Extract the gmlid from the filename\n",
    "gmlid = file_name.replace(\"rectified_facade_\", \"\").replace(\".jpg\", \"\")\n",
    "\n",
    "# Search for the corresponding row\n",
    "row = df[df[\"gmlid\"] == gmlid]\n",
    "\n",
    "if not row.empty:\n",
    "    # Retrieve relevant facade height and width\n",
    "    facade_height_meters = float(row[\"relevant_facade_height\"].values[0])\n",
    "    facade_width_meters = float(row[\"relevant_facade_width\"].values[0])\n",
    "\n",
    "    # Compute facade area\n",
    "    facade_area = facade_height_meters * facade_width_meters\n",
    "\n",
    "    rich.print(f\"GMLID: {gmlid}\")\n",
    "    rich.print(f\"Facade Height: {facade_height_meters} meters\")\n",
    "    rich.print(f\"Facade Width: {facade_width_meters} meters\")\n",
    "    rich.print(f\"Facade Area: {facade_area} square meters\")\n",
    "else:\n",
    "    rich.print(f\"No matching GMLID found for {gmlid}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### image area in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Image Height: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">272</span> pixels\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Image Height: \u001b[1;36m272\u001b[0m pixels\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Image Width: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">337</span> pixels\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Image Width: \u001b[1;36m337\u001b[0m pixels\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total image area for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91664</span> pixels²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total image area for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m91664\u001b[0m pixels²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Find the image details for the given file name\n",
    "image_details = next(item for item in data[\"images\"] if item[\"file_name\"] == file_name)\n",
    "\n",
    "# Extract width and height\n",
    "image_height_pixels = image_details[\"height\"]\n",
    "image_width_pixels = image_details[\"width\"]\n",
    "\n",
    "rich.print(f\"Image Height: {image_height_pixels} pixels\")\n",
    "rich.print(f\"Image Width: {image_width_pixels} pixels\")\n",
    "\n",
    "# Calculate the image area\n",
    "image_area_pixels = image_height_pixels * image_width_pixels\n",
    "\n",
    "# rich.print(f\"Image dimensions for '{file_name}': {image_width}x{image_height}\")\n",
    "rich.print(f\"Total image area for '{file_name}': {image_area_pixels} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pixel size in square meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">pixel size in meters <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.001116041188306421</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "pixel size in meters \u001b[1;36m0.001116041188306421\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the pixel size in square meters\n",
    "pixel_size_meters = (facade_width_meters / image_width_pixels) * (\n",
    "        facade_height_meters / image_height_pixels)\n",
    "\n",
    "rich.print(\"pixel size in meters\", pixel_size_meters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ground truth area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total window area for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21784.693692249522</span> pixels²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total window area for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m21784.693692249522\u001b[0m pixels²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_id = next(item[\"id\"] for item in data[\"images\"] if item[\"file_name\"] == file_name)\n",
    "\n",
    "# Step 2: Filter annotations for the image_id and category \"window\" (category_id = 1)\n",
    "window_annotations = [\n",
    "    annotation for annotation in data[\"annotations\"]\n",
    "    if annotation[\"image_id\"] == image_id and annotation[\"category_id\"] == 1\n",
    "]\n",
    "\n",
    "# Step 3: Calculate total area and count\n",
    "windows_area_gt = sum(ann[\"area\"] for ann in window_annotations)\n",
    "num_windows = len(window_annotations)\n",
    "\n",
    "# rich.print(f\"Number of windows: {num_windows}\")\n",
    "rich.print(f\"Total window area for '{file_name}': {windows_area_gt} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the predicted area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\mohamad\\WWR\\Evaluation_subset\\rectified_facade_DENW11AL0000h3Gt.jpg: 320x384 8 windows, 60.2ms\n",
      "Speed: 1.1ms preprocess, 60.2ms inference, 2.5ms postprocess per image at shape (1, 3, 320, 384)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total predicted windows area for <span style=\"color: #008000; text-decoration-color: #008000\">'Evaluation_subset\\rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15991.98046875</span> pixels²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total predicted windows area for \u001b[32m'Evaluation_subset\\rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m15991.98046875\u001b[0m pixels²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run prediction\n",
    "results = model.predict(source=image_path, conf=0.2)\n",
    "\n",
    "# Extract bounding boxes\n",
    "boxes = results[0].boxes.xyxy  # Get boxes in (x1, y1, x2, y2) format\n",
    "scores = results[0].boxes.conf  # Get confidence scores\n",
    "\n",
    "# Convert to a PyTorch tensor (if not already)\n",
    "if not isinstance(boxes, torch.Tensor):\n",
    "    boxes = torch.tensor(boxes)\n",
    "if not isinstance(scores, torch.Tensor):\n",
    "    scores = torch.tensor(scores)\n",
    "\n",
    "# Set confidence threshold\n",
    "threshold = 0.1\n",
    "valid_boxes = boxes[scores > threshold]\n",
    "\n",
    "# Get the count of valid boxes\n",
    "num_valid_boxes = valid_boxes.shape[0]\n",
    "\n",
    "# Calculate the area of each valid bounding box\n",
    "areas = (valid_boxes[:, 2] - valid_boxes[:, 0]) * (valid_boxes[:, 3] - valid_boxes[:, 1])\n",
    "\n",
    "# Sum the areas to get total window area\n",
    "windows_area_p = areas.sum().item()\n",
    "\n",
    "# Print the total windows area\n",
    "rich.print(f\"Total predicted windows area for '{image_path}': {windows_area_p} pixels²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR actual and predicted in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Window-to-Wall Ratio <span style=\"font-weight: bold\">(</span>ground truth<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>WWR<span style=\"font-weight: bold\">)</span> for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2377</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Window-to-Wall Ratio \u001b[1m(\u001b[0mground truth\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mWWR\u001b[1m)\u001b[0m for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m0.2377\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Window-to-Wall Ratio <span style=\"font-weight: bold\">(</span>predcited<span style=\"font-weight: bold\">)</span> <span style=\"font-weight: bold\">(</span>WWR<span style=\"font-weight: bold\">)</span> for <span style=\"color: #008000; text-decoration-color: #008000\">'rectified_facade_DENW11AL0000h3Gt.jpg'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1745</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Window-to-Wall Ratio \u001b[1m(\u001b[0mpredcited\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mWWR\u001b[1m)\u001b[0m for \u001b[32m'rectified_facade_DENW11AL0000h3Gt.jpg'\u001b[0m: \u001b[1;36m0.1745\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate ground truth WWR\n",
    "wwr_gt = windows_area_gt / image_area_pixels if image_area_pixels > 0 else 0\n",
    "\n",
    "rich.print(f\"Window-to-Wall Ratio (ground truth) (WWR) for '{file_name}': {wwr_gt:.4f}\")\n",
    "\n",
    "# Calculate predicted WWR\n",
    "wwr_P = windows_area_p / image_area_pixels if image_area_pixels > 0 else 0\n",
    "\n",
    "rich.print(f\"Window-to-Wall Ratio (predcited) (WWR) for '{file_name}': {wwr_P:.4f}\")\n",
    "\n",
    "# Calculate the absolute difference\n",
    "wwr_difference = abs(wwr_gt - wwr_P)\n",
    "\n",
    "# Optionally, calculate percentage difference\n",
    "wwr_percentage_diff = (wwr_difference / wwr_gt * 100) if wwr_gt > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "# rich.print(f\"[bold]Difference between ground truth and predicted WWR:[/bold] {wwr_difference:.4f}\")\n",
    "# rich.print(f\"[bold]Percentage difference:[/bold] {wwr_percentage_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR actual in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">facade in meters <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102.30079948491976</span> meters²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "facade in meters \u001b[1;36m102.30079948491976\u001b[0m meters²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "facade_meters = image_area_pixels * pixel_size_meters\n",
    "\n",
    "rich.print(f\"facade in meters {facade_meters} meters²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">windows in meters ground trught <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.31261543518955</span> meters²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "windows in meters ground trught \u001b[1;36m24.31261543518955\u001b[0m meters²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_meters_GT = windows_area_gt * pixel_size_meters\n",
    "\n",
    "rich.print(f\"windows in meters ground trught {window_meters_GT} meters²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">WWR Ground trugth <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.23765811760614333</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "WWR Ground trugth \u001b[1;36m0.23765811760614333\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WWR_GT_actual = window_meters_GT / facade_meters\n",
    "\n",
    "rich.print(f\"WWR Ground trugth {WWR_GT_actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WWR predicted in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">windows in meters predcited <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.847708885716823</span> meters²\n",
       "</pre>\n"
      ],
      "text/plain": [
       "windows in meters predcited \u001b[1;36m17.847708885716823\u001b[0m meters²\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_meters_P = windows_area_p * pixel_size_meters\n",
    "\n",
    "rich.print(f\"windows in meters predcited {window_meters_P} meters²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">WWR predicted <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.17446304403855384</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "WWR predicted \u001b[1;36m0.17446304403855384\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WWR_p_actual = window_meters_P / facade_meters\n",
    "\n",
    "rich.print(f\"WWR predicted {WWR_p_actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the results (yolo) on the Evalution subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and visualize the image with predictions\n",
    "annotated_image = results[0].plot()  # Plot the results on the image\n",
    "\n",
    "# Display the image using Matplotlibs\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(annotated_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
